{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H1>Breaking Down Hackers News Posts:</H1>\n",
    "\n",
    "<H2>Goal:</H2>\n",
    "\n",
    "Using a subset of 20,000 out of 300,000, weâ€™ll be exploring and comparing two types of the post, Show HN and Ask HN, from the Hacker New, to find the answer to the following questions:\n",
    "\n",
    "<ol>\n",
    "  <li>Do Ask HN or Show HN receive more comments on average?\n",
    "</li>\n",
    "  <li>Do posts created at a certain time receive more comments on average?</li>\n",
    "</ol> \n",
    "\n",
    " \n",
    "<H2>Background:</H2>\n",
    "\n",
    "<a href=\"https://news.ycombinator.com/\">Hacker News:</a> Hacker News is a site started by the startup incubator Y Combinator, where user-submitted stories (known as \"posts\") are voted and commented upon, similar to Reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "<b>Source: Dataquest: Guided Project: Exploring Hacker News Posts</b>\n",
    " \n",
    " <H2>Definitions:</H2>\n",
    " \n",
    "Ask HN: A post that users submit posts to ask the Hacker News community a specific question, such as \"What is the best online course you've ever taken?\"\n",
    "Show HN: A post that users submit posts to show the Hacker News community a project, product, or just generally something interesting\n",
    "\n",
    "<H2>Data Selection:</H2>\n",
    "\n",
    "Data set was reduced from 300,00 rows to 20,000 with excluding all submissions that did not receive any comments, and then randomly sampling from the remaining submissions.\n",
    "\n",
    "The following dataset can be found <a href=\"https://www.kaggle.com/hacker-news/hacker-news-posts\">here</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Data Cleaning Process Overview</H1>\n",
    "\n",
    "<ol>\n",
    "<li>Importing Data</li>\n",
    "<li>Remove the Headers</li>\n",
    "<li>Extracting Ask HN and Show HN Posts</li>\n",
    "<li>Calculating the Average Number of Comments for Ask HN and Show HN Posts</li>\n",
    "<li>Finding the Amount of Ask Posts and Comments by Hour Created</li>\n",
    "<li>Calculating the Average Number of Comments for Ask HN Posts by Hour</li>\n",
    "<li>Sorting and Printing Values from a List of Lists</li>\n",
    "<li> Formatting</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Importing Data</H2>\n",
    "\n",
    "In this step, we'll import the dataset sample of 20,000 records of posts that has one comment or more. Then print out only 6 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'], ['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "open = open('hacker_news.csv')\n",
    "file= reader(open)\n",
    "hn=list(file)\n",
    "\n",
    "#Print out the first six row\n",
    "print(hn[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Remove the Headers</H2>\n",
    "\n",
    "This process will remove the header only to manipulate the data but before doing so I will place the header in a header variable to be used as a reference and below is a reference for clarity. \n",
    "\n",
    "<ul>\n",
    "<li> index 0, id: The unique identifier from Hacker News for the post</li>\n",
    "<li> index 1, title: The title of the post</li>\n",
    "<li> index 2, url: The URL that the posts links to, if it the post has a URL</li>\n",
    "<li> index 3, num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes</li>\n",
    "<li>index 4, num_comments: The number of comments that were made on the post</li>\n",
    "<li>index 5, author: The username of the person who submitted the post</li>\n",
    "<li>index 6, created_at: The date and time at which the post was submitted</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "[['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']]\n"
     ]
    }
   ],
   "source": [
    "headers = hn[0]\n",
    "hn=hn[1:]\n",
    "print(headers)\n",
    "print(hn[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H2>Extracting Ask HN and Show HN Posts</H2>\n",
    "\n",
    "By isolating the dataset in three categories Ask, Show and others make it easier to get the total sum of comment by the hour and total average of comments by the hour. Using the string ask_ hn and show_hn to extract the post for each data list. To confirm our code work I will be print out each list length and the total list length which should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744 1162 17194 20100\n",
      "Data lists match total list\n"
     ]
    }
   ],
   "source": [
    "ask_posts=[]\n",
    "show_posts=[]\n",
    "other_posts=[]\n",
    "\n",
    "for row in hn:\n",
    "    title=row[1].lower()\n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "print(len(ask_posts),len(show_posts),len(other_posts), len(hn))\n",
    "\n",
    "if (len(ask_posts)+len(show_posts)+len(other_posts))== len(hn):\n",
    "    print(\"Data lists match total list\")\n",
    "else:\n",
    "    print(\"Data lists dose not match total list\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Calculating the Average Number of Comments for Ask HN and Show HN Posts</H2>\n",
    "\n",
    "Parsing the main list into three separate categories makes it easier to aggregate the information in getting the average comments per post. But I also would like to know the average points per post as well so instead of rewriting the code for three different instances I will code a List_average function with the arguments of list and index, improve readability and reuse code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This functions take the list and index to get the aveage if quantifiable \n",
    "#othewise error will occure\n",
    "def avg_per_post(a_posts, index):\n",
    "    total=0\n",
    "    for post in a_posts:\n",
    "        total += int(post[index])\n",
    "    AvgPerPost = total / len(a_posts)\n",
    "    return AvgPerPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "# Get the aveage comment per post  for ask comments\n",
    "avg_ask_comments =avg_per_post(ask_posts, 4)\n",
    "print(avg_ask_comments)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "# Get the aveage comment per post  for show comments\n",
    "avg_show_comments =avg_per_post(show_posts, 4)\n",
    "print(avg_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Finding the Amount of Ask Posts and Comments by Hour Created</H2>\n",
    "\n",
    "Now that we can see that the average comments in ask posts list are higher than the show post list we can focus on the questions on how many comments are submitted per hour and what time of day the most comments are submitted for the ask HN data sets.\n",
    "\n",
    "Using index 6, the created_at column as the argument for the DateTime module will be able to find on what hour comments were submitted.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': 447,\n",
       " '01': 683,\n",
       " '02': 1381,\n",
       " '03': 421,\n",
       " '04': 337,\n",
       " '05': 464,\n",
       " '06': 397,\n",
       " '07': 267,\n",
       " '08': 492,\n",
       " '09': 251,\n",
       " '10': 793,\n",
       " '11': 641,\n",
       " '12': 687,\n",
       " '13': 1253,\n",
       " '14': 1416,\n",
       " '15': 4477,\n",
       " '16': 1814,\n",
       " '17': 1146,\n",
       " '18': 1439,\n",
       " '19': 1188,\n",
       " '20': 1722,\n",
       " '21': 1745,\n",
       " '22': 479,\n",
       " '23': 543}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "# first getting the date submitted and number of comments per post\n",
    "for post in ask_posts:\n",
    "    result_list.append([post[6], int(post[4])])\n",
    "\n",
    "comments_by_hour = {}\n",
    "counts_by_hour = {}\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    hour = dt.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    hour=hour.strftime(\"%H\")\n",
    "    if hour in counts_by_hour:\n",
    "        comments_by_hour[hour] += row[1]\n",
    "        counts_by_hour[hour] += 1\n",
    "    else:\n",
    "        comments_by_hour[hour] = row[1]\n",
    "        counts_by_hour[hour] = 1\n",
    "\n",
    "comments_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Calculating the Average Number of Comments for Ask HN Posts by Hour</H2>\n",
    "\n",
    "Using the hour as a key for both comments_by_hour  and counts_by_hour dictionary we can get the average number of comments by the hour. Where  counts_by_hour is used as the denominator and the comments_by_hour is used as the numerator. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['04', 7.17],\n",
       " ['21', 16.01],\n",
       " ['08', 10.25],\n",
       " ['03', 7.8],\n",
       " ['17', 11.46],\n",
       " ['13', 14.74],\n",
       " ['19', 10.8],\n",
       " ['02', 23.81],\n",
       " ['09', 5.58],\n",
       " ['18', 13.2],\n",
       " ['05', 10.09],\n",
       " ['23', 7.99],\n",
       " ['10', 13.44],\n",
       " ['16', 16.8],\n",
       " ['15', 38.59],\n",
       " ['12', 9.41],\n",
       " ['20', 21.52],\n",
       " ['06', 9.02],\n",
       " ['00', 8.13],\n",
       " ['07', 7.85],\n",
       " ['01', 11.38],\n",
       " ['11', 11.05],\n",
       " ['22', 6.75],\n",
       " ['14', 13.23]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hr in comments_by_hour:# using the round function to be readable\n",
    "    avg_by_hour.append([hr, round((comments_by_hour[hr] / counts_by_hour[hr]),2)])\n",
    "\n",
    "avg_by_hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sorting and Printing Values from a List of Lists</h2>\n",
    "\n",
    "As we print out the result of calculation we can see that is out of order and sorting the list would give us a better insight as to when the most post is. Unforutanly we are no able to use the sort function as it sorts the hour and not average with the given example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['23', 7.99],\n",
       " ['22', 6.75],\n",
       " ['21', 16.01],\n",
       " ['20', 21.52],\n",
       " ['19', 10.8],\n",
       " ['18', 13.2],\n",
       " ['17', 11.46],\n",
       " ['16', 16.8],\n",
       " ['15', 38.59],\n",
       " ['14', 13.23],\n",
       " ['13', 14.74],\n",
       " ['12', 9.41],\n",
       " ['11', 11.05],\n",
       " ['10', 13.44],\n",
       " ['09', 5.58],\n",
       " ['08', 10.25],\n",
       " ['07', 7.85],\n",
       " ['06', 9.02],\n",
       " ['05', 10.09],\n",
       " ['04', 7.17],\n",
       " ['03', 7.8],\n",
       " ['02', 23.81],\n",
       " ['01', 11.38],\n",
       " ['00', 8.13]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_test=sorted(avg_by_hour, reverse=True)\n",
    "sort_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this issue we can swap the index where the value is index 0 and the hour is index 1, which will allow us to use the sort function to give the desired results.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[38.59, '15'],\n",
       " [23.81, '02'],\n",
       " [21.52, '20'],\n",
       " [16.8, '16'],\n",
       " [16.01, '21'],\n",
       " [14.74, '13'],\n",
       " [13.44, '10'],\n",
       " [13.23, '14'],\n",
       " [13.2, '18'],\n",
       " [11.46, '17'],\n",
       " [11.38, '01'],\n",
       " [11.05, '11'],\n",
       " [10.8, '19'],\n",
       " [10.25, '08'],\n",
       " [10.09, '05'],\n",
       " [9.41, '12'],\n",
       " [9.02, '06'],\n",
       " [8.13, '00'],\n",
       " [7.99, '23'],\n",
       " [7.85, '07'],\n",
       " [7.8, '03'],\n",
       " [7.17, '04'],\n",
       " [6.75, '22'],\n",
       " [5.58, '09']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1], row[0]])\n",
    "    \n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "\n",
    "sorted_swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Formatting</h2>\n",
    "\n",
    "The last step in our data cleaning process is the format the result readable way. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for 'Ask HN' Comments\n",
      "15:00 : 38.59 average comments per post\n",
      "02:00 : 23.81 average comments per post\n",
      "20:00 : 21.52 average comments per post\n",
      "16:00 : 16.80 average comments per post\n",
      "21:00 : 16.01 average comments per post\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Hours for 'Ask HN' Comments\")\n",
    "temp=\"{} : {:.2f} average comments per post\"\n",
    "for avg, hr in sorted_swap[:5]:\n",
    "    print(temp.format(dt.strptime(hr, \"%H\").strftime(\"%H:%M\"),avg) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion:</h2>\n",
    "\n",
    "We see that Ask HN receives more comments on average of 14 per post compare to 10 comments per post for Show HN. Which given the list size of both, we can estimate that for Ask HN post would generate up to 24, 000 comments in total while show HN post would generate up to 11, 000 comments in total. Given the figures, if we want to Maximus interaction among user we should form our content in a question format. As to when we should post our content that data shows that we should post at the time between 1500 and 1600 or 3:00 and 4:00  as with the combined time span will allow us to cove  48% of when the most average comment is generated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataquest/system/env/python3/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1744"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only titles for ask_hn\n",
    "title=[]\n",
    "for row in ask_posts:\n",
    "    title.append(row[1])\n",
    "len(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Extra:</H1>\n",
    "I was curious to see if there were any buzz words among the ask_posts, by using  sick it k-clustering method that group words and is the first step to classification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataquest/system/env/python3/lib/python3.4/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " learning\n",
      " machine\n",
      " resources\n",
      " hn\n",
      " ask\n",
      " online\n",
      " peer\n",
      " swift\n",
      " chinese\n",
      " ios\n",
      "Cluster 1:\n",
      " use\n",
      " app\n",
      " hn\n",
      " ask\n",
      " web\n",
      " framework\n",
      " payment\n",
      " android\n",
      " tracking\n",
      " data\n",
      "Cluster 2:\n",
      " programming\n",
      " business\n",
      " start\n",
      " ask\n",
      " hn\n",
      " language\n",
      " learn\n",
      " languages\n",
      " idea\n",
      " today\n",
      "Cluster 3:\n",
      " does\n",
      " hn\n",
      " ask\n",
      " company\n",
      " use\n",
      " passwords\n",
      " hate\n",
      " matter\n",
      " job\n",
      " manage\n",
      "Cluster 4:\n",
      " job\n",
      " hn\n",
      " ask\n",
      " leave\n",
      " new\n",
      " passion\n",
      " salary\n",
      " work\n",
      " quit\n",
      " money\n",
      "Cluster 5:\n",
      " best\n",
      " learn\n",
      " hn\n",
      " ask\n",
      " way\n",
      " place\n",
      " modern\n",
      " book\n",
      " 2016\n",
      " code\n",
      "Cluster 6:\n",
      " project\n",
      " working\n",
      " worth\n",
      " hn\n",
      " ask\n",
      " open\n",
      " ideas\n",
      " source\n",
      " idea\n",
      " management\n",
      "Cluster 7:\n",
      " work\n",
      " hn\n",
      " ask\n",
      " ideas\n",
      " does\n",
      " hours\n",
      " day\n",
      " project\n",
      " app\n",
      " did\n",
      "Cluster 8:\n",
      " startup\n",
      " good\n",
      " ask\n",
      " hn\n",
      " idea\n",
      " advice\n",
      " joining\n",
      " need\n",
      " man\n",
      " family\n",
      "Cluster 9:\n",
      " hn\n",
      " ask\n",
      " software\n",
      " using\n",
      " did\n",
      " web\n",
      " new\n",
      " source\n",
      " open\n",
      " 2016\n"
     ]
    }
   ],
   "source": [
    "X=title\n",
    "pip = Pipeline([('tf',TfidfVectorizer(stop_words='english')),])\n",
    "\n",
    "X_df = pip.fit_transform(X).todense()\n",
    "pca = PCA(n_components= 3).fit(X_df)\n",
    "data2d =pca.transform(X_df)\n",
    "\n",
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k)\n",
    "model.fit(X_df)\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "cen_2d=pca.transform(centroids)\n",
    "terms = pip.get_params()['tf'].get_feature_names()\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to generate buzz word but I was able to get a sense of the general concept of the questions being asked like programing language and life management questions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
